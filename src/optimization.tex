\section{Optimization \& Adv.\ Robustness}
\transitionFrame{Relating Optimization \& Adversarial Guarantees}


\begin{frame}{What is an adversarial \blue{guarantee}?}
  \begin{itemize}[<+->]
    \setlength{\itemsep}{20pt}
    \item \textbf{Guarantee}: Certifies that a ``well-defined'' class of adversarial attacks are prevented
      \begin{itemize}
        \item \textit{Example}: Class of \textbf{\blue{first-order adversaries}}
      \end{itemize}

    \item Last week we reviewed multiple possible defensive techniques, e.g.,~robustness, disinformation,~etc.

    \item \textbf{Question}: What guarantees did the authors' of last week's paper provide?
    \vspace{-14pt}
    \item \textbf{Answer}: None. \onslide<+->{\textit{And that's the point}}

    \item \textbf{Goal}: Make \textbf{\green{provable statements about robustness}}.
      \begin{itemize}[<+->]
        \item If we can do it, that's a \textit{yuge} deal.
        \item Ends\textsuperscript{*} the arms race.
      \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}{Nomenclature}
  \onslide<+->{Quite standard and used throughout this talk.}  \onslide<+->{\textit{If you have a question,} \textbf{ask now!}}

  \begin{itemize}[<+->]
    \setlength{\itemsep}{6pt}
  \item ${\X \in \domainX \subset \mathbb{R}^d}$: Feature vector
    \item ${\y \in \domainY \equiv \ints{k}}$: Ground truth (true) label
    \item $\func{\distr}{\domainX \times \domainY}{\mathbb{R}_{{\geq}0}}$: Underlying sample data probability distribution s.t.\ ${(\X,\y) \sim \distr}$

    \vspace{6pt}
    \item ${\params \in \domainP}$: Model (neural network) parameters

    \vspace{6pt}
  \item ${\sPerturb \subseteq \mathbb{R}^{d}}$: Set of allowed (adversarial) perturbations can be applied to any~$\X$
    \item ${\perturb \in \sPerturb}$: (Adversarial) perturbation s.t.
      \begin{equation}\label{eq:AdversarialX}
        \X^{\text{adv}} = x + \delta
      \end{equation}

    \vspace{6pt}
    \item $\mathbb{E}_{x \in \mathcal{X}}\sbrack{f(x)}$: Expected value (mean) of $f(x)$
  \end{itemize}
\end{frame}


\begin{frame}{``Standard'' Training Model}
  \begin{itemize}[<+->]
    \setlength{\itemsep}{20pt}
    \item Neural network training is based on \textbf{\blue{empirical risk minimization}} (ERM)
    \item \textbf{\green{Fundamental Idea}}:
      \begin{equation}\label{eq:ERM}
        \min_{\params} \mathbb{E}_{(\X,\y) \sim \distr} \sbrack{\loss \left( \X, \y ; \params \right)}
      \end{equation}

    \item \textbf{\blue{Attack Model}}: For any ${x \in \domainX}$, adversary can make a set of allowed perturbations~$\sPerturb$
      \begin{itemize}
        % \item \textit{Theoretical Paradigm}: $\sPerturb = \linf\text{-ball}$
        \item We'll formally define $\sPerturb$ later.  For now, it is any subset of $\mathbb{R}^{d}$
      \end{itemize}

    \item \textbf{\red{Big Problem}}: ERM-trained models are not robust to adversarially perturbed examples~\cite{Biggio:2013,Szegedy:2013}
  \end{itemize}
\end{frame}


\subsection{Minimax}
\begin{frame}{Adversarial Training Framework}
  \begin{definition}
    \blue{\textit{Minimax optimization}} reformulates ERM as:
    \onslide<2->{
      \begin{equation}\label{eq:Minimax}
        \textcolor<5->{ForestGreen}{\min_{\params} \rho(\params)}\text{, where } \rho(\params) = \mathbb{E}_{(\X,\y) \sim \distr} \sbrack{\textcolor<4->{red}{ \max_{\delta \in \sPerturb} \loss (\X + \perturb, \y ; \params)}} \text{.}
      \end{equation}
    }
  \end{definition}

  \vspace{8pt}
  \only<3-5>{Optimization Framework's Two Key Parts
    \begin{itemize}
      \setlength{\itemsep}{4pt}
    \item \onslide<4->{\textbf{\color{red}Inner Maximization}: Adversary finds ${\xadv = \X + \perturb}$ with highest loss (think worst case)}
      \item \onslide<5->{\textbf{\color{ForestGreen}Outer Minimization}: Defender selects model parameters~$\params$ that minimize adversarial loss}
    \end{itemize}
  }
  \only<6->{
    \textbf{Question}: What can we say if ${\textcolor{ForestGreen}{\min_{\params} \rho(\params)} \rightarrow 0}$?

    \vspace{4pt}
    \onslide<7->{\textbf{Answer}: \green{Perfect robustness}. \textit{Adversarial guarantee} no perturbation defined in attack model ($\sPerturb$) fools the network.}
    \begin{itemize}
      \item \onslide<8->{\textbf{Summary}: No adversarial examples!\textsuperscript{*}}
    \end{itemize}

    \vspace{10pt}
    \onslide<9->{\textbf{Why?}} \onslide<10->{Inner maximization considers \textbf{worst case} adversary guaranteeing all cases}
   }
\end{frame}


\begin{frame}{So\ldots Are We Done Here?}
  \onslide<+->{}
  \onslide<+->{\textbf{No.}} \onslide<+->{Inner \& outer components of minimax formulation are non-convex and non-concave respectively.}
  \begin{itemize}[<+->]
    \item \textit{Worst Case}: $\text{Non-convexity} \implies \text{Intractable}$ (rule of thumb)
  \end{itemize}
  \vfill
  \onslide<+->{\textbf{\green{Key Contribution \#1}}: Demonstrating adversarial minimax optimization \textit{is} tractable.}
  % \begin{itemize}[<+->]
  %   \item We will talk about tractability in the coming slides.
  % \end{itemize}
\end{frame}
